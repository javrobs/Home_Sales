# PySpark exercise
## Overview
For this challenge we used SparkSQL to determine key metrics about home sales data. We used Spark to create temporary views, partition the data, cache and uncache a temporary table, and verify that the table has been uncached.
## Results
* Regular PySpark structure:  
![result_using_pyspark](images/ss_1.jpg)

* Cached table:  
![result_using_cache](images/ss_2.jpg)

* Parquet and partition by year:  
![parquet_plus_partition](images/ss_3.jpg)